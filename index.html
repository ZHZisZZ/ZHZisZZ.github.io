<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Zhanhui Zhou</title>

  <!-- Minimum HTML style -->
  <link rel="stylesheet" href="https://latex.vercel.app/style.min.css" />
  <link rel="stylesheet" href="https://latex.vercel.app/prism/prism.css" />
  <style>
    body {
      max-width: 100ch;
    }
    /* Center title and paragraph */
    .about,
    .about p {
      text-align: left;
      margin-top: 0;
    }
    .about {
      margin: 2.25rem 0;
    }
    .about > h2 {
      font-size: 1rem;
      margin-bottom: -0.2rem;
    }
  </style>
  
  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <style>
    .icon-container {
        display: flex;
        gap: 20px;
        justify-content: center;
        align-items: center;
        margin-top: 20px;
    }
    .icon-container a {
        text-decoration: none;
        color: #333;
        font-size: 2em;
    }
    .icon-container a:hover {
        color: #007bff;
    }
  </style>

  <!-- expansion -->
  <style>
    .expandable {cursor: pointer;}

    .content {display: none;}

    .expandable.active + .content {display: block;}
  </style>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
        var expandables = document.querySelectorAll('.expandable');
        expandables.forEach(function(expandable) {
            expandable.addEventListener('click', function() {
                this.classList.toggle('active');
            });
        });
    });
  </script>
</head>

<body id="top">
  <header>
    <h1>ZHANHUI ZHOU</h1>
  </header>

  <div class="icon-container">
    <!-- Email Icon -->
    <a href="mailto:asap.zzhou@gmail.com" target="_blank">
        <i class="fas fa-envelope"></i>
    </a>

    <!-- Google Scholar Icon -->
    <a href="https://scholar.google.com/citations?user=SbACfYQAAAAJ&hl=zh-CN" target="_blank">
      <i class="fas fa-graduation-cap"></i>
    </a>

    <!-- GitHub Icon -->
    <a href="https://github.com/ZHZisZZ" target="_blank">
        <i class="fab fa-github"></i>
    </a>

    <!-- CV Icon -->
    <a href="src/zzh_cv.pdf" target="_blank">
        <i class="fas fa-file-alt"></i>
    </a>
</div>

  <div class="about">
    <p>
      I earned dual bachelor's degrees from <a href="https://eecs.engin.umich.edu/">UMich</a> 
      and <a href="https://en.sjtu.edu.cn/">SJTU</a>; now I am an incoming CS PhD student at UC Berkeley. 
      My research is broadly situated in the field of <b>human-centered NLP</b>, where I focus on developing 
      (1) scalable algorithms that align language models with human values and 
      (2) interactive systems that enhance human experiences by leveraging stronger language models and continuously improving through human interaction. 
      Here is my <a href="src/zzh_cv.pdf">CV</a>.
      <br>
    </p>
  </div>

  <main>
    <article>
      

      <h2>Selected Publications</h2>
      <font color="grey">Please see <a href="https://scholar.google.com/citations?user=SbACfYQAAAAJ&hl=zh-CN" style="color: grey;">Google Scholar</a> for a complete and up-to-date list of publications </font><br>
      <div style="margin-bottom: 15px;">
        <b>VLMs can Aggregate Scattered Training Patches</b><br>
        <b>Zhanhui Zhou</b>, Lingjie Chen, Chao Yang, Chaochao Lu<br>
        Preprint, under review<br>
        <a href="https://github.com/ZHZisZZ/visual-stitching/blob/main/assets/visual-stitching.pdf">PAPER</a>&nbsp<a href="https://github.com/ZHZisZZ/visual-stitching">CODE</a>
      </div>
      <div style="margin-bottom: 15px;">
        <b>Emergent Response Planning in LLMs</b><br>
        Zhichen Dong<sup>*</sup>, <b>Zhanhui Zhou<sup>*</sup></b>, Zhixuan Liu, Chao Yang, Chaochao Lu<br>
        ICML 2025<br>
        <a href="https://arxiv.org/abs/2502.06258">PAPER</a>
      </div>
      <div style="margin-bottom: 15px;">
        <b>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</b><br>
        <b>Zhanhui Zhou</b>, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao<br>
        NeurIPS 2024<br>
        <a href="https://arxiv.org/abs/2405.19262">PAPER</a>&nbsp<a href="https://github.com/ZHZisZZ/weak-to-strong-search">CODE</a>
      </div>
      <div style="margin-bottom: 15px;">
        <b>Emulated Disalignment: Safety Alignment for Large Langugae Models May Backfire!</b><br>
        <b>Zhanhui Zhou</b>, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao<br>
        ACL 2024, <font color="red"><b>Outstanding Paper Award (&lt; 1% of all submissions)</b></font><br>
        <a href="https://arxiv.org/abs/2402.12343">PAPER</a>&nbsp<a href="https://github.com/ZHZisZZ/emulated-disalignment">CODE</a>
      </div>
      <div style="margin-bottom: 15px;">
        <b>Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization</b><br>
        <b>Zhanhui Zhou<sup>*</sup></b>, Jie Liu<sup>*</sup>, Chao Yang, Jing Shao, Xiangyu Yue, Wanli Ouyang, Yu Qiao<br>
        ACL 2024 Findings<br>
        <a href="https://arxiv.org/abs/2310.03708">PAPER</a>&nbsp<a href="https://github.com/ZHZisZZ/modpo">CODE</a>
      </div>
      <div style="margin-bottom: 15px;">
        <b>INTENT: Interactive Tensor Transformation Synthesis</b><br>
        <b>Zhanhui Zhou<sup>*</sup></b>, Man To Tang<sup>*</sup>, Qiping Pan<sup>*</sup>, Shangyin Tan, Xinyu Wang, Tianyi Zhang<br>
        UIST 2022<br>
        <a href="https://dl.acm.org/doi/abs/10.1145/3526113.3545653">PAPER</a>&nbsp<a href="https://github.com/ZHZisZZ/INTENT">CODE</a>
      </div>

      <div class="expandable">See all â–¼</div>
      <div class="content">
        <div style="margin-bottom: 15px;">
          <b>Inference-Time Language Model Alignment via Integrated Value Guidance</b><br>
          Zhixuan Liu<sup>*</sup>, <b>Zhanhui Zhou<sup>*</sup></b>, Yuanfu Wang, Chao Yang, Yu Qiao<br>
          EMNLP 2024 Findings</b><br>
          <a href="https://arxiv.org/abs/2409.17819">PAPER</a>&nbsp
        </div>
        <div style="margin-bottom: 15px;">
          <b>MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues</b><br>
          Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, <b>Zhanhui Zhou</b>, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang<br>
          ACL 2024</b><br>
          <a href="https://arxiv.org/abs/2402.14762">PAPER</a>&nbsp<a href="https://github.com/mtbench101/mt-bench-101">CODE</a>
        </div>
        <div style="margin-bottom: 15px;">
          <b>ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models</b><br>
          Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, <b>Zhanhui Zhou</b>, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng<br>
          ACL 2024 Findings<br>
          <a href="https://arxiv.org/abs/2402.14660">PAPER</a>&nbsp<a href="https://github.com/conceptmath/conceptmath">CODE</a>
        </div>
        <div style="margin-bottom: 15px;">
          <b>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</b><br>
          Zhichen Dong<sup>*</sup>, <b>Zhanhui Zhou<sup>*</sup></b>, Chao Yang, Jing Shao, Yu Qiao<br>
          NAACL 2024<br>
          <a href="https://arxiv.org/abs/2402.09283">PAPER</a>&nbsp<a href="https://github.com/niconi19/LLM-conversation-safety">PAPER LIST</a>
        </div>
      </div>
    </article>
  </main>

  

</body>

</html>
